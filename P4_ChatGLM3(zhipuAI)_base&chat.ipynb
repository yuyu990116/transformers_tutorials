{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2C4R1qwJhgX7fsj4NqLLj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuyu990116/transformers_tutorials/blob/main/P4_ChatGLM3(zhipuAI)_base%26chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8qkxklVQCJ7"
      },
      "outputs": [],
      "source": [
        "先加载chat模型查看数据格式（chatglm3-6b而不是chatglm3-base)模型\n",
        "使用 ?model.chat 代码  会返回文件，点击文件进去源码  or  进入模型所在文件夹查看源码\n",
        "chatglm3对special token进行了特殊处理，没法通过tokenize方式得到，只能使用get_command得到token（千问也对special token进行了特殊处理）避免decode混淆\n",
        "chatglm3有一个后处理的功能（因为它有functioncall和执行代码的功能）"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basemodel推荐把embedding层也训练，这里只训练了注意力层"
      ],
      "metadata": {
        "id": "fyWvgoFHVR-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GLM的chat格式：在base的基础上进行chat的训练，V1是prefix训练，V2是CausalLM训练，prefix训练在多轮对话上效率不高（因为要把QAQA都拆开来训练）\n",
        "chatglm3支持function call（工具调用）和code interpreter（代码执行）\n"
      ],
      "metadata": {
        "id": "q6QLK4g3YoPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#使用chat模型查看数据格式：\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"d:/Pretrained_models/ZhipuAI/chatglm3-6b/\", trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"d:/Pretrained_models/ZhipuAI/chatglm3-6b/\", trust_remote_code=True, low_cpu_mem_usage=True, torch_dtype=torch.half, device_map=\"auto\")\n",
        "model.chat(tokenizer, \"考试的技巧有哪些？\", history=[])"
      ],
      "metadata": {
        "id": "-IWGdO8KaN2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "?model.chat    #查看源码，通过源码确定数据需要被处理成为的格式"
      ],
      "metadata": {
        "id": "mV4fbs9PxQeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "?tokenizer.build_chat_input  #从源码看到了build_chat_input,这里就具体查看这个方法"
      ],
      "metadata": {
        "id": "CbOA03NixXp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer(\"<|user|>\", add_special_tokens=False))\n",
        "print(tokenizer.get_command(\"<|user|>\")) #确定special token的取用方法（tokenizer.get_command）"
      ],
      "metadata": {
        "id": "J3b5QFXNxePo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.build_chat_input(\"考试的技巧有哪些？\", history=[], role=\"user\")"
      ],
      "metadata": {
        "id": "T6CFM1nbxfiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PIdxtGpe1K0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([64790, 64792, 64795, 30910,    13, 30910, 32227, 54530, 33741, 34953,\n",
        "         31514, 64796])"
      ],
      "metadata": {
        "id": "irZ3xeGpxowz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "数据格式：\n",
        "\n",
        "[gMASK]sop<|user|> \\n Prompt<|assistant|> \\n Response eos_token\n",
        "后面这个\\n Response eos_token 是因为GLM3有后处理，会split('\\n')来分割metadata(function名)和context(模型的回答) 在这里我们先不考虑functioncall和执行代码那部分的训练，因此用空的加一个\\n来隔开response"
      ],
      "metadata": {
        "id": "a-RGuVAcxqJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#确定格式后对base model进行训练：\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
        "import torch\n",
        "from peft import LoraConfig, TaskType, get_peft_model, PeftModel"
      ],
      "metadata": {
        "id": "qNG2TWWsxuSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"zhengr/alpaca-chinese-dataset\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"d:/Pretrained_models/ZhipuAI/chatglm3-6b-base/\", trust_remote_code=True)\n",
        "print(tokenizer(tokenizer.eos_token), tokenizer.eos_token_id) #确定eostoken的取用方法（tokenizer.eos_token_id）"
      ],
      "metadata": {
        "id": "u6lQg0SV1DSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_func(example):\n",
        "    MAX_LENGTH = 256\n",
        "    input_ids, attention_mask, labels = [], [], []\n",
        "    query = \"\\n\".join([example[\"instruction\"], example[\"input\"]]).strip()     # query\n",
        "    query = tokenizer.build_chat_input(query, history=[], role=\"user\")  # 格式变为[gMASK]sop<|user|> \\n query<|assistant|> 并且是tensor形式\n",
        "    response = tokenizer(\"\\n\" + example[\"output\"], add_special_tokens=False)  # \\n response, 缺少eos token；如果没有add_special_tokens=False就会在<|assistant|>后面又来一次[gMASK]sop\n",
        "    input_ids = query[\"input_ids\"][0].numpy().tolist() + response[\"input_ids\"] + [tokenizer.eos_token_id] #去掉query的tensor格式，加上eos tokend\n",
        "    attention_mask = query[\"attention_mask\"][0].numpy().tolist() + response[\"attention_mask\"] + [1] #这个1是给eos token的\n",
        "    labels = [-100] * len(query[\"input_ids\"][0].numpy().tolist()) + response[\"input_ids\"] + [tokenizer.eos_token_id]\n",
        "    if len(input_ids) > MAX_LENGTH:\n",
        "        input_ids = input_ids[:MAX_LENGTH]\n",
        "        attention_mask = attention_mask[:MAX_LENGTH]\n",
        "        labels = labels[:MAX_LENGTH]\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)"
      ],
      "metadata": {
        "id": "L_GA8I4n2RDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#查看process_func是否正确\n",
        "print(tokenizer.decode(tokenized_ds[1][\"input_ids\"]))\n",
        "print(tokenizer.decode(list(filter(lambda x: x != -100, tokenized_ds[1][\"labels\"]))))"
      ],
      "metadata": {
        "id": "Mym3dKzM4OIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"d:/Pretrained_models/ZhipuAI/chatglm3-6b-base/\", trust_remote_code=True, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(name)\n",
        "#查看模型都有哪些层"
      ],
      "metadata": {
        "id": "IRr1--HN4RX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#开始lora微调：\n",
        "config = LoraConfig(target_modules=[\"query_key_value\"], modules_to_save=[\"post_attention_layernorm\"])  #basemodel推荐把embedding层也训练，这里只训练了注意力层\n",
        "print(config)\n",
        "model = get_peft_model(model, config)\n",
        "for name, parameter in model.named_parameters():\n",
        "    print(name)\n",
        "print(model.print_trainable_parameters())\n",
        "print(model)  #这里应该有lora层了，此时model是peftmodel\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./chatbot\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=16,\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=1e-4, #这里也可以调小一点\n",
        "    remove_unused_columns=False,\n",
        "    save_strategy=\"epoch\"\n",
        ")#因为在LoraConfig没有指定tasktype（task_type=TaskType.CAUSAL_LM），所以model作为peftmodel接受参数的形式是元组或者字典，必须加一句remove_unused_columns=False，否则训练的时候将无法正确接受参数\n",
        "#源码中peftcausalmodel接受参数形式是指定了各参数的名称的，所以不需要加一句remove_unused_columns=False\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_ds.select(range(6000)),\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
        ")\n",
        "trainer.train()\n",
        "model.eval()\n",
        "print(model.chat(tokenizer, \"考试的技巧有哪些？\", history=[])[0])"
      ],
      "metadata": {
        "id": "9kEQrQ_VOCsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9X4QuJoHRUd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#量化8bit训练（显存占用减少，但训练速度会变慢）：  int8:-127 —— +127   需要安装bitsandbytes\n",
        "#llama int8: 与llama之前的模型训练前面都一样，只是在model=要添加一个参数（load_in_8bit)：\n",
        "model = AutoModelForCausalLM.from_pretrained(\"D:/Pretrained_models/modelscope/Llama-2-7b-ms\", low_cpu_mem_usage=True,\n",
        "                                             torch_dtype=torch.half, device_map=\"auto\", load_in_8bit=True)\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.shape, param.dtype) #检验是否量化成功\n",
        "model.config #会多一个quantization_config\n",
        "#由于训练是int8，但是原模型是float32，所以不建议merge，而是使用PEFT调用"
      ],
      "metadata": {
        "id": "vrIOUWpvRUlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chatglm3 int8：同理\n",
        "model = AutoModelForCausalLM.from_pretrained(\"d:/Pretrained_models/ZhipuAI/chatglm3-6b-base/\", trust_remote_code=True, low_cpu_mem_usage=True,\n",
        "                                             torch_dtype=torch.half, device_map=\"auto\", load_in_8bit=True)\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.dtype)"
      ],
      "metadata": {
        "id": "-Z0OAQ3HtiUl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}