{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMff3saBRfQNwTLN6mPRSFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuyu990116/transformers_tutorials/blob/main/P3_llama_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrST9ADG8X2K"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/nlp\")\n",
        "!pip install datasets\n",
        "!pip install accelerate==0.22.0\n",
        "!pip install transformers==4.33.1\n",
        "!pip install peft==0.5.0\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM,DataCollatorForSeq2Seq,TrainingArguments,Trainer,pipeline\n",
        "from datasets import Dataset,load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "import torch\n",
        "ds = load_dataset(\"zhengr/alpaca-chinese-dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install modelscope\n",
        "from modelscope.hub.snapshot_download import snapshot_download\n",
        "snapshot_download(model_id=\"Shanghai_AI_Laboratory/internlm-20b\", cache_dir=\"/content/drive/MyDrive/Pretrained_models\")"
      ],
      "metadata": {
        "id": "xzCjtjwzAlxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/Pretrained_models/Shanghai_AI_Laboratory/internlm-20b\",low_cpu_mem_usage=True,torch_dtype=torch.half)\n",
        "#low_cpu_mem_usage=True,torch_dtype=torch.half会节省空间\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/Pretrained_models/Shanghai_AI_Laboratory/internlm-20b\")"
      ],
      "metadata": {
        "id": "VY6cJZeg-ziN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer\n",
        "tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "Uy24VO1UsoYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llama2的默认paddingside是左边，经过data_process处理后会出问题\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token_id = 2\n",
        "tokenizer"
      ],
      "metadata": {
        "id": "W8_bye3ssXEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(example): #这次数据处理不进行batched，只处理单个的数据，因为label部分不太容易做\n",
        "  max_length=256 # Llama分词器没针对中文进行训练，它会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
        "  #tokenizer这里要设置add_special_tokens=False\n",
        "  tokenized_input=tokenizer(\"\\n\".join([\"User:\"+example[\"instruction\"],example[\"input\"]]).strip()+\"\\nAssistant:\", add_special_tokens=False)\n",
        "  tokenized_output=tokenizer(example[\"output\"], add_special_tokens=False) #不能在这里把eos_token跟文本放在一起后直接送入tokenizer，不然会导致eos_token在解码的时候无法被解成结束标识符\n",
        "  input_ids=tokenized_input[\"input_ids\"]+tokenized_output[\"input_ids\"] + [tokenizer.eos_token_id]\n",
        "  attention_mask=tokenized_input[\"attention_mask\"]+tokenized_output[\"attention_mask\"] + [1] #加的这个1是给eostoken用的\n",
        "  labels= [-100]*len(tokenized_input[\"input_ids\"])+tokenized_output[\"input_ids\"]+ [tokenizer.eos_token_id]\n",
        "  if len(input_ids)>max_length:\n",
        "    input_ids=input_ids[:max_length]\n",
        "    attention_mask=attention_mask[:max_length]\n",
        "    labels=labels[:max_length]\n",
        "  return {\n",
        "      \"input_ids\":input_ids,\n",
        "      \"attention_mask\":attention_mask,\n",
        "      \"labels\":labels\n",
        "  }"
      ],
      "metadata": {
        "id": "pHf8-fkQ8jt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)\n",
        "tokenized_ds"
      ],
      "metadata": {
        "id": "nqbnN2Gn1gBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_ds[0][\"input_ids\"])"
      ],
      "metadata": {
        "id": "diJxbJzW2087"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenized_ds[0][\"input_ids\"])"
      ],
      "metadata": {
        "id": "8VbTROXJ23eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_ds[1][\"labels\"])))"
      ],
      "metadata": {
        "id": "IcqreyGk285x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(task_type=TaskType.CAUSAL_LM,)\n",
        "config"
      ],
      "metadata": {
        "id": "ftryE3bj3Cb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, config)\n",
        "config\n"
      ],
      "metadata": {
        "id": "qOqxyOaH3WqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#虽然之前加载model的时候指定了torch_dtype，但是用peftmodel加载以后，lora的部分还没有转成半精度，所以需要再转一次\n",
        "model = model.half()\n",
        "#此时整个模型都是半精度，而优化方法如果想要使用adam,那就需要将adam_epsilon调大（默认1e-8) 可以改成1e-4"
      ],
      "metadata": {
        "id": "c2VFzNVh3qD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.enable_input_require_grads() # 设置gradient_checkpointing=True时，要执行该方法"
      ],
      "metadata": {
        "id": "Czm0U1XA3xC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# args = TrainingArguments(\n",
        "#     output_dir=\"./chatbot\",\n",
        "#     per_device_train_batch_size=2,\n",
        "#     gradient_accumulation_steps=8,\n",
        "#     logging_steps=10,\n",
        "#     num_train_epochs=1,\n",
        "# )"
      ],
      "metadata": {
        "id": "zqc3uTYc6fXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"./llama_lora\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "    adam_epsilon=1e-4\n",
        "    logging_steps=5,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=5\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_ds[\"train\"].select(range(6000)),\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "IXiqaxiH6gx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_model = PeftModel.from_pretrained(model, model_id=\"/content/drive/MyDrive/nlp/llama_lora/checkpoint-10\")\n",
        "p_model\n",
        "\n",
        "p_model = p_model.cuda()\n",
        "ipt = tokenizer(\"Human: {}\\n{}\".format(\"考试有哪些技巧？\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors=\"pt\").to(p_model.device)\n",
        "tokenizer.decode(p_model.generate(**ipt,max_length=128)[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "mgvqplt_66Yj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}