{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1OgoBupuPzE0-LtoTG9jrJq46psorWGH7",
      "authorship_tag": "ABX9TyN2J44Qrw361sd8iziH73N8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "784bb6c01b314cd8a3d0fe311529043d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d358fac42ca4998a1664d7da8de1194",
              "IPY_MODEL_1d291f39eea44cf48f4d4c57459c3fed",
              "IPY_MODEL_745bfeddd8804461a5b3074a60f69332"
            ],
            "layout": "IPY_MODEL_835ccd6da0204d739de301c02debc810"
          }
        },
        "3d358fac42ca4998a1664d7da8de1194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad0dc31f18294dfba0d80e1894cf29b6",
            "placeholder": "​",
            "style": "IPY_MODEL_b36c84cbd16d44e197370ecb8f0032d0",
            "value": "Map: 100%"
          }
        },
        "1d291f39eea44cf48f4d4c57459c3fed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94f0869d301642eebf8af3250281b340",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6674827c49e42a9a2a70e16fee8c46e",
            "value": 1000
          }
        },
        "745bfeddd8804461a5b3074a60f69332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_974b5cf7f550484092902093614bbf4e",
            "placeholder": "​",
            "style": "IPY_MODEL_bb48c6b583684fb78bf0c46c95f9883a",
            "value": " 1000/1000 [00:01&lt;00:00, 1072.66 examples/s]"
          }
        },
        "835ccd6da0204d739de301c02debc810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad0dc31f18294dfba0d80e1894cf29b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b36c84cbd16d44e197370ecb8f0032d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94f0869d301642eebf8af3250281b340": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6674827c49e42a9a2a70e16fee8c46e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "974b5cf7f550484092902093614bbf4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb48c6b583684fb78bf0c46c95f9883a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuyu990116/transformers_tutorials/blob/main/P3_llama_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrST9ADG8X2K"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/nlp\")\n",
        "!pip install datasets\n",
        "!pip install accelerate==0.22.0\n",
        "!pip install transformers==4.33.1\n",
        "!pip install peft==0.5.0\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM,DataCollatorForSeq2Seq,TrainingArguments,Trainer,pipeline\n",
        "from datasets import Dataset,load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "import torch\n",
        "ds = load_dataset(\"zhengr/alpaca-chinese-dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxsgVgN0JSdf",
        "outputId": "e83d82b8-8815-4d5f-e9b0-ec3945a25a64"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'output', 'en_output', 'en_input', 'en_instruction', 'input'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install modelscope\n",
        "# from modelscope.hub.snapshot_download import snapshot_download\n",
        "# snapshot_download(model_id=\"skyline2006/llama-7b\", cache_dir=\"/content/drive/MyDrive/Pretrained_models\")"
      ],
      "metadata": {
        "id": "xzCjtjwzAlxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/Pretrained_models/skyline2006/llama-7b\",low_cpu_mem_usage=True,torch_dtype=torch.half,device_map='auto')\n",
        "#low_cpu_mem_usage=True,torch_dtype=torch.half会节省空间\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/Pretrained_models/skyline2006/llama-7b\", unk_token=\"<unk>\")\n",
        "#如果不加unk_token=\"<unk>\"在加载tokenizer的时候会报错超过最大循环深度"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY6cJZeg-ziN",
        "outputId": "2da8bb07-38a0-4cf4-e6c0-944529204e3c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.pad_token_id)\n",
        "print(tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "Uy24VO1UsoYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9697a2-85b6-4f47-85c1-0523f6a8f533"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3bL36GiHR6q",
        "outputId": "c68e454f-a2ac-4204-dfb6-2e559895e890"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaTokenizerFast(name_or_path='/content/drive/MyDrive/Pretrained_models/skyline2006/llama-7b', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZN-l-CbDJKNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(example): #这次数据处理不进行batched，只处理单个的数据，因为label部分不太容易做\n",
        "  max_length=256 # Llama分词器没针对中文进行训练，它会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
        "  #tokenizer这里要设置add_special_tokens=False\n",
        "  tokenized_input=tokenizer(\"\\n\".join([\"User:\"+example[\"instruction\"],example[\"input\"]]).strip()+\"\\nAssistant:\", add_special_tokens=False)\n",
        "  tokenized_output=tokenizer(example[\"output\"], add_special_tokens=False) #不能在这里把eos_token跟文本放在一起后直接送入tokenizer，不然会导致eos_token在解码的时候无法被解成结束标识符\n",
        "  input_ids=tokenized_input[\"input_ids\"]+tokenized_output[\"input_ids\"] + [tokenizer.eos_token_id]\n",
        "  attention_mask=tokenized_input[\"attention_mask\"]+tokenized_output[\"attention_mask\"] + [1] #加的这个1是给eostoken用的\n",
        "  labels= [-100]*len(tokenized_input[\"input_ids\"])+tokenized_output[\"input_ids\"]+ [tokenizer.eos_token_id]\n",
        "  if len(input_ids)>max_length:\n",
        "    input_ids=input_ids[:max_length]\n",
        "    attention_mask=attention_mask[:max_length]\n",
        "    labels=labels[:max_length]\n",
        "  return {\n",
        "      \"input_ids\":input_ids,\n",
        "      \"attention_mask\":attention_mask,\n",
        "      \"labels\":labels\n",
        "  }"
      ],
      "metadata": {
        "id": "pHf8-fkQ8jt-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds = ds.map(data_process, remove_columns=ds['train'].column_names)\n",
        "tokenized_ds"
      ],
      "metadata": {
        "id": "nqbnN2Gn1gBw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "784bb6c01b314cd8a3d0fe311529043d",
            "3d358fac42ca4998a1664d7da8de1194",
            "1d291f39eea44cf48f4d4c57459c3fed",
            "745bfeddd8804461a5b3074a60f69332",
            "835ccd6da0204d739de301c02debc810",
            "ad0dc31f18294dfba0d80e1894cf29b6",
            "b36c84cbd16d44e197370ecb8f0032d0",
            "94f0869d301642eebf8af3250281b340",
            "b6674827c49e42a9a2a70e16fee8c46e",
            "974b5cf7f550484092902093614bbf4e",
            "bb48c6b583684fb78bf0c46c95f9883a"
          ]
        },
        "outputId": "b9ae20cc-c717-4a82-ebac-5f1c07438072"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "784bb6c01b314cd8a3d0fe311529043d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#llama2的默认paddingside是左边，经过data_process处理后会出问题\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.pad_token_id = 2\n",
        "tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "W8_bye3ssXEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d409bfe-48b9-4486-fd02-ec3aa7eaf4e2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_ds['train'][0][\"input_ids\"])"
      ],
      "metadata": {
        "id": "diJxbJzW2087",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20578ca-fd03-4fe0-867f-e5dd7dbea862"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4911, 29901, 30672, 31381, 30847, 31502, 232, 138, 146, 31022, 30325, 31190, 30486, 31704, 30275, 30210, 30406, 30716, 31180, 30882, 13, 7900, 22137, 29901, 29871, 29896, 29889, 29871, 30785, 30406, 31669, 30716, 30210, 233, 183, 132, 232, 136, 186, 30214, 30847, 233, 186, 142, 233, 184, 183, 232, 153, 186, 31584, 30503, 30716, 31300, 31584, 30267, 13, 29906, 29889, 30785, 30406, 30716, 234, 158, 137, 31391, 233, 164, 185, 31997, 30893, 30613, 232, 189, 176, 232, 189, 162, 30716, 30214, 31507, 30847, 233, 183, 154, 234, 165, 154, 30503, 233, 183, 154, 233, 193, 164, 30267, 13, 29941, 29889, 29871, 31302, 30528, 30564, 30467, 31669, 30716, 31474, 235, 178, 137, 30267, 13, 29946, 29889, 29871, 233, 166, 131, 31213, 31624, 30397, 30503, 234, 132, 143, 233, 189, 140, 31185, 31675, 30210, 233, 191, 146, 30716, 30993, 232, 137, 184, 30214, 31666, 31436, 30594, 31273, 31810, 30267, 13, 29945, 29889, 29871, 233, 186, 142, 233, 184, 183, 30594, 31016, 235, 193, 134, 234, 162, 176, 30214, 30785, 30406, 231, 192, 145, 31151, 31180, 233, 186, 142, 233, 184, 183, 232, 153, 186, 31584, 30651, 31669, 234, 189, 169, 30406, 30716, 30267, 13, 29953, 29889, 31997, 30893, 236, 158, 171, 30716, 31666, 30998, 31149, 30406, 30909, 232, 158, 176, 235, 140, 189, 31391, 31149, 31221, 31838, 236, 168, 177, 30406, 30895, 30210, 30267, 13, 29955, 29889, 29871, 232, 139, 186, 234, 140, 156, 31391, 233, 183, 154, 30880, 30594, 31057, 236, 154, 176, 30716, 31300, 31584, 30267, 13, 29947, 29889, 29871, 232, 138, 146, 31022, 31999, 31710, 232]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sivzbAVYNoXi",
        "outputId": "924c5487-0777-4292-c3c1-e61bea393ead"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': '我们如何减少日常生活中的用水量？',\n",
              " 'output': '1. 使用节水的洁具，如淋浴喷头和水龙头。\\n2.使用水盆或桶收集家庭废水，例如洗碗和洗澡。\\n3. 提高社区节水意识。\\n4. 检查管道和灌溉系统的漏水情况，并及时修复。\\n5. 淋浴时间较短，使用低流量淋浴喷头以节约用水。\\n6.收集雨水并将其用于园艺或其他非饮用目的。\\n7. 刷牙或洗手时关闭水龙头。\\n8. 减少给草坪浇水的时间。\\n9. 尽可能重复使用灰水（洗衣机、浴室水槽和淋浴的水）。\\n10. 只购买节能洗碗机和洗衣机。',\n",
              " 'en_output': '1. Use water-efficient fixtures like showerheads and faucets. \\n2. Use a basin or bucket to collect household wastewater tasks such as washing dishes and taking baths. \\n3. Raise awareness of water-saving practices in our community. \\n4. Check for water leaks in plumbing and irrigation systems and repair them promptly. \\n5. Take shorter showers and use low flow showerheads to save water. \\n6. Collect rainwater and use it for gardening or other non-drinking purposes. \\n7. Turn off the tap when you are brushing your teeth or soaping your hands. \\n8. Reduce the time you water your lawn. \\n9. Reuse graywater (water from washing machine, bathroom sinks, and showers) as much as possible.\\n10. Buy only energy-efficient dishwashers and washing machines.',\n",
              " 'en_input': '',\n",
              " 'en_instruction': 'How can we reduce water usage in our daily lives?',\n",
              " 'input': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0T30D8hKccy",
        "outputId": "e3a8c3d3-0b70-40c4-9913-06bf4a673161"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': '编辑文章，使其对读者更具吸引力。',\n",
              " 'output': '自主机器人是计算机控制的机器，经过编程以在没有任何人工输入的情况下执行特定任务，从而实现新的效率、准确性和可靠性水平。自主机器人越来越多地用于各种行业，从制造业，它们可以精确和一致的质量组装复杂的组件，到医疗保健，在那里它们可以协助医疗测试和程序，到安全，在那里它们可以监控大面积并确保人员和财产安全。自主机器人还可以减少错误并提高危险或危险环境中的安全性，例如在工业过程的检查或维修期间。由于其多功能性，自主机器人将彻底改变我们的工作方式，使我们能够使任务更简单、更快，并最终更愉快。',\n",
              " 'en_output': 'Autonomous robots are computer-controlled machines that are programmed to carry out a specific task without any human input, enabling new levels of efficiency, accuracy and reliability. Autonomous robots are increasingly used in a variety of industries, from manufacturing, where they can assemble complex components with precision and consistent quality, to healthcare, where they can assist with medical tests and procedures, to security, where they can monitor large areas and keep people and property safe. Autonomous robots can also reduce errors and increase safety in dangerous or hazardous environments, such as during inspections or repairs of industrial processes. Thanks to their versatility, autonomous robots are set to revolutionize the way we work, enabling us to make tasks simpler, faster and ultimately, more enjoyable.',\n",
              " 'en_input': 'Autonomous robots are computer-controlled machines that are programmed to carry out a specific task without any human input. Autonomous robots are increasingly used in a variety of industries, from manufacturing to healthcare to security.',\n",
              " 'en_instruction': 'Edit the article to make it more engaging for the readers.',\n",
              " 'input': '自主机器人是计算机控制的机器，被编程为在没有任何人工输入的情况下执行特定任务。自主机器人越来越多地用于各种行业，从制造业到医疗保健再到安全。'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenized_ds['train'][0][\"input_ids\"])"
      ],
      "metadata": {
        "id": "8VbTROXJ23eR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "35b1d675-1f63-4cc9-dc63-e8b6c353262f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'User:我们如何减少日常生活中的用水量？\\nAssistant: 1. 使用节水的洁具，如淋浴喷头和水龙头。\\n2.使用水盆或桶收集家庭废水，例如洗碗和洗澡。\\n3. 提高社区节水意识。\\n4. 检查管道和灌溉系统的漏水情况，并及时修复。\\n5. 淋浴时间较短，使用低流量淋浴喷头以节约用水。\\n6.收集雨水并将其用于园艺或其他非饮用目的。\\n7. 刷牙或洗手时关闭水龙头。\\n8. 减少给草�'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_ds['train'][1][\"labels\"])))"
      ],
      "metadata": {
        "id": "IcqreyGk285x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "659cc074-62bf-4698-dd1e-f40f358b3d7c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'自主机器人是计算机控制的机器，经过编程以在没有任何人工输入的情况下执行特定任务，从而实现新的效率、准确性和可靠性水平。自主机器人越来越多地用于各种行业，从制造业，它们可以精确和一致的质量组装复杂的组件，到医疗保健，在那里它们可以��'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(task_type=TaskType.CAUSAL_LM,)\n",
        "config"
      ],
      "metadata": {
        "id": "ftryE3bj3Cb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c939feb8-1450-4827-e8fb-9ebc003d19ec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, config)\n",
        "config\n"
      ],
      "metadata": {
        "id": "qOqxyOaH3WqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a327132-1d93-4470-b01c-d2233d7f682c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/content/drive/MyDrive/Pretrained_models/skyline2006/llama-7b', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules=['q_proj', 'v_proj'], lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#虽然之前加载model的时候指定了torch_dtype，但是用peftmodel加载以后，lora的部分还没有转成半精度，所以需要再转一次\n",
        "model = model.half()\n",
        "#此时整个模型都是半精度，而优化方法如果想要使用adam,那就需要将adam_epsilon调大（默认1e-8) 可以改成1e-4"
      ],
      "metadata": {
        "id": "c2VFzNVh3qD5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAbRt88xK09a",
        "outputId": "7a3421ad-fff3-4f47-c8af-6beb9d9af221"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, parameter in model.named_parameters():\n",
        "    print(parameter.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZujMb2JJK9pB",
        "outputId": "2213d1c6-e470-4534-88a9-363c33d4a902"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n",
            "torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.enable_input_require_grads() # 设置gradient_checkpointing=True时，要执行该方法"
      ],
      "metadata": {
        "id": "Czm0U1XA3xC5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# args = TrainingArguments(\n",
        "#     output_dir=\"./chatbot\",\n",
        "#     per_device_train_batch_size=2,\n",
        "#     gradient_accumulation_steps=8,\n",
        "#     logging_steps=10,\n",
        "#     num_train_epochs=1,\n",
        "# )"
      ],
      "metadata": {
        "id": "zqc3uTYc6fXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"./llama_lora\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "    adam_epsilon=1e-4,\n",
        "    logging_steps=5,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=5\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_ds[\"train\"].select(range(999)),\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "IXiqaxiH6gx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_model = PeftModel.from_pretrained(model, model_id=\"/content/drive/MyDrive/nlp/llama_lora/checkpoint-10\")\n",
        "p_model\n",
        "\n",
        "p_model = p_model.cuda()\n",
        "ipt = tokenizer(\"Human: {}\\n{}\".format(\"我们如何减少日常生活中的用水量？\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors=\"pt\").to(p_model.device)\n",
        "tokenizer.decode(p_model.generate(**ipt,max_length=128,do_sample=True,repetition_penalty=1.1)[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "mgvqplt_66Yj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "ee60a2c9-ad8f-49d3-934e-8ee3ed2d8ab3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: 我们如何减少日常生活中的用水量？\\n\\nAssistant: 为了保持一般的生活，我们可能会觉得在使用个人电视或计算机时要用热水。这通常是为了当中提供与网友共享好东西。\\nHuman: How do we reduce regular daily water usage?\\n\\nAssistant: In order to maintain the usual living, we may feel that we need hot water supply when using'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yGHDxhAANUpY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}